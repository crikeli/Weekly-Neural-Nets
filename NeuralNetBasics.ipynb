{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding a Basic 2 Layer Neural Network\n",
    "\n",
    "#### This ipynb is written to assist my personal understanding of neural networks. It would make me even happier if it helped others understand also :) . I will go in-depth with every part of the code as I write it.\n",
    "\n",
    "## Step 1 - importing dependencies :\n",
    "\n",
    "- The only dependency we will need for running this network is numpy (for mathematically manipulating multidimensional arrays and matrices.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Defining the non-linearity :\n",
    "\n",
    "- For this example, I have used the sigmoid non-linearity function. It maps any value to a value between 0 & 1. This is very helpful as it helps convert numbers to probabilities.\n",
    "\n",
    "\n",
    "##### A very helpful property of sigmoid function is that its output can be used to create its derivative. For example if th sigmoid has an output of \"x\", then the derivative is calculated simply as x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# non linearity that is utilized to gauge output confidence.\n",
    "# values are between 0 & 1. The sigmoid function can also generate\n",
    "# a derivative if (deriv=True)\n",
    "def sigmoidFn(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# def nonlin(x,deriv=False):\n",
    "#     if (deriv==True):\n",
    "#         return x*(1-x)\n",
    "#     return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Initializing the input matrix using numpy :\n",
    "\n",
    "- For all intents and purposes, I will be creating a very basic input-dataset. It is a simple 6 X 3 matrix (6 rows by 3 columns)\n",
    "\n",
    "\n",
    "- Each row is a \"training-example\" and each column is an input \"node\" that is fed into the network. This means, the network has 3 inputs and 6 training examples per input which the network can be trained from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input matrix where every row is a training example and each column is\n",
    "# an input node to the network.\n",
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [0,1,0],\n",
    "              [1,0,1],\n",
    "              [1,1,1],\n",
    "              [1,1,0]])\n",
    "\n",
    "# print X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Initializing the output dataset using numpy :\n",
    "\n",
    "- In accordance with the simple training data, I deviced a simple output (so that the network can be quickly trained), which is a 1 X 6 (1 row by 6 columns) array.\n",
    "\n",
    "\n",
    "- The next step would be transpose the array, which would change the shape of the matrix above to a 6 X 1. This is done to ensure the output is in accordance with the input. i.e, Each row is a training example and each column is an output node.\n",
    "### The above statement means that the output layer has 6 inputs and one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output dataset where each row is a training example\n",
    "# .T transposes the matrix, so [0,0,0,1,1,1] becomes [[0,\n",
    "#                                                      0,\n",
    "#                                                      0,\n",
    "#                                                      1,\n",
    "#                                                      1,\n",
    "#                                                      1]]\n",
    "\n",
    "Y = np.array([[0,0,0,1,1,1]]).T\n",
    "\n",
    "# print Y\n",
    "\n",
    "# y = np.array([[1,1,0,0]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Random seeding \n",
    "\n",
    "- Seeding is a numpy method so that there is a uniform random distribution of numbers everytime the network is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# print np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Defining a weight Matrix for this Neural Network :\n",
    "\n",
    "- Because there are only 2 layers in the neural network, the input layer & the output layer, only one weight matrix is necessary to connect the two layers.\n",
    "\n",
    "- The dimension of the weight matrix is 3 X 1 because the input layer(as discussed in step 3) is of size 3 and the output layer(as discussed in step 4) is a size of 1.\n",
    "\n",
    "- The initialization of the weight is very theory intensive, but as best practice for this simple example, we initialize the weight randomly with a zero mean.\n",
    "    ##### \"zero mean\" means that the sum of the entries vector divided by the dimension of the matrix equates to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing the weight matrix randomly.\n",
    "# weightMatrix = 2*np.random.random((3,1)) - 1\n",
    "# print weightMatrix\n",
    "weightMatrix = 2*np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Training the Neural Network :\n",
    "\n",
    "- We create a \"for\" loop that iterates many times over our training data defined in step 3 & 4 to optimize the network to the specified dataset.\n",
    "\n",
    "### Forward Propagation :\n",
    "\n",
    "- We are going to utilize a technique known as full-batch training which will process all the input data(defined as X in step 3) at the same time.\n",
    "\n",
    "- We then proceed towards letting the netword predict (I use the term predict loosely because each prediction is a constant iteration towards an even better prediction) the output based on the input.\n",
    "\n",
    "- The next bit of code below can be interpreted in 2 steps. Firstly, our inputLayer matrix, which is X(defined in step 3) of size 6 X 3 undergoes dot product (matrix multiplication) with our initialized weightMatrix(from step 6) of size 3 X 1. As a rule, the result of the dot product is a matrix of size 6 X 1.\n",
    "```python \n",
    "outputLayer = sigmoidFn(np.dot(inputLayer,weightMatrix))\n",
    "```\n",
    "\n",
    "- Another important thing occurance to note in the above line of code is that the output of the dot product passes through the sigmoidFn declared way up in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for iter in xrange(100000):\n",
    "\n",
    "    # forward propagation\n",
    "    inputLayer = X\n",
    "    outputLayer = sigmoidFn(np.dot(inputLayer,weightMatrix))\n",
    "#     print outputLayer\n",
    "#     print \"W-Mtrix:\",weightMatrix\n",
    "#     print \"iputLayer\",inputLayer\n",
    "#     print \"DotProd\",np.dot(inputLayer, weightMatrix)\n",
    "#     print \"SigmoidFnCalc:\",sigmoidFn(np.dot(inputLayer, weightMatrix))\n",
    "#     print \"Output Layer:\",outputLayer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 8 - Evaluating Calculation Error :\n",
    "\n",
    "\n",
    "- The line of code below is simply a means to calculate the accuracy of our predicted value(outputLayer) compared to the actual, predefined output value (Y).\n",
    "- Remember, that Y is a 6 X 1 matrix(As defined in part 4). The outputLayer is also a 6 X 1 matrix, therefore it is fairly intuitive that in order to calculate the error, we would have to subtract the two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.325416  ]\n",
      " [ 0.42596688]\n",
      " [ 0.60603278]\n",
      " [ 0.4634023 ]\n",
      " [ 0.57052986]\n",
      " [ 0.73360785]]\n"
     ]
    }
   ],
   "source": [
    "outputLayerError = Y - outputLayer\n",
    "    # print Y\n",
    "print outputLayer\n",
    "    # print \"O-put Error\",outputLayerError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - \n",
    "\n",
    "- To understand this step fully, we can take it step-by-step.\n",
    "    - Firstly, after calculating the derivative sigmoidFn(outputLayer, True), each calculated value is always between 0 & 1. That is a property of the sigmoid function.\n",
    "    - Secondly, we multiply the returned 6 X 1 derivative consisting matrix above to the outputLayerError, which is also a 6 X 1 matrix.\n",
    "###### What the second step does is that it multiplies the derivatves to the error \"elementwise\" and that helps to reduce the errors of predictions that carry a high confidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.325416  ]\n",
      " [ 0.42596688]\n",
      " [ 0.60603278]\n",
      " [ 0.4634023 ]\n",
      " [ 0.57052986]\n",
      " [ 0.73360785]]\n"
     ]
    }
   ],
   "source": [
    "errorBasedChange = outputLayerError * sigmoidFn(outputLayer, True) \n",
    "print outputLayer\n",
    "    # print weightMatrix\n",
    "    # print outputLayerError\n",
    "    # print \"Derivs\", sigmoidFn(inputLayer, True)\n",
    "    # print errorBasedChange\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Updating the weight matrix based on previously achieved results.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT [[ 0.325416  ]\n",
      " [ 0.42596688]\n",
      " [ 0.60603278]\n",
      " [ 0.4634023 ]\n",
      " [ 0.57052986]\n",
      " [ 0.73360785]]\n"
     ]
    }
   ],
   "source": [
    "#     print \"weightMatrix1\",weightMatrix\n",
    "#     print \"IPUT LAYER\",inputLayer\n",
    "#     print \"EBC\",errorBasedChange\n",
    "weightMatrix += np.dot(inputLayer.T, errorBasedChange)\n",
    "    \n",
    "#     print \"DOTPROD\", np.dot(inputLayer.T, errorBasedChange)\n",
    "#     print \"WMAtrix2\", weightMatrix\n",
    "#     print \"weigh:\",updatedWeights\n",
    "\n",
    "print \"OUTPUT\", outputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
