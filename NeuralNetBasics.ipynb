{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding a Basic 2 Layer Neural Network\n",
    "\n",
    "#### This ipynb is written to assist my personal understanding of neural networks. It would make me even happier if it helped others understand also :) . I will go in-depth with every part of the code as I write it. For added ease of use, anyone can follow along as test the code by commenting out print statements to understand what exactly is going in each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - importing dependencies :\n",
    "\n",
    "- The only dependency we will need for running this network is numpy (for mathematically manipulating multidimensional arrays and matrices.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Defining the non-linearity :\n",
    "\n",
    "- For this example, I have used the sigmoid non-linearity function. It maps any value that is passed in the function to a value between 0 & 1. This is very helpful as it helps convert numbers to probabilities.\n",
    "- The sigmoid function can generate derivative of a sigmoid when the derivative is set to \"True\".\n",
    "\n",
    "##### A very helpful property of sigmoid function is that its output can be used to create its derivative. For example if the sigmoid has an output of \"x\", then the derivative is calculated simply as x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# non linearity that is utilized to gauge output confidence.\n",
    "# values are between 0 & 1. The sigmoid function can also generate\n",
    "# a derivative if (deriv=True)\n",
    "def sigmoidFn(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Uncomment the code below to see slope calculation in action as we provide an input(x1) array.\n",
    "# x1 = np.array([[2],[0],[-1]])\n",
    "\n",
    "# When we pass x1 values through the sigmoidFunction, we get corresponding points(y1) on the sigmoid graph.\n",
    "# y1 = sigmoidFn(x1, False)\n",
    "\n",
    "# The values that are returned above(y1) can now be used to calculate the derivative(slopes) of those points\n",
    "# slopes = sigmoidFn(y1, True)\n",
    "\n",
    "# print \"x1 values: \", x1\n",
    "# print \"y1 Values: \",y1\n",
    "# print \"slope values: \",slopes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Initializing the input matrix using numpy :\n",
    "\n",
    "- For all intents and purposes, I will be creating a very basic input-dataset. It is a simple 6 X 3 matrix (6 rows by 3 columns)\n",
    "\n",
    "\n",
    "- Each row is a \"training-example\" and each column is an input \"node\" that is fed into the network. This means, the network has 3 inputs and 6 training examples per input which the network can be trained from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input matrix where every row is a training example and each column is\n",
    "# an input node to the network.\n",
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1],\n",
    "              [1,1,1],\n",
    "              [1,1,0]])\n",
    "\n",
    "# print X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Initializing the output matrix using numpy :\n",
    "\n",
    "- In accordance with the simple training data, I deviced a simple output (so that the network can be quickly trained), which is a 1 X 6 (1 row by 6 columns) array.\n",
    "\n",
    "\n",
    "- The next step would be transpose the array, which would change the shape of the matrix above to a 6 X 1. This is done to ensure the output is in accordance with the input. i.e, Each row is a training example and each column is an output node.\n",
    "### The above statement means that the output layer has 6 inputs and one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output dataset where each row is a training example\n",
    "# .T transposes the matrix, so [0,0,0,1,1,1] becomes [[0,\n",
    "#                                                      0,\n",
    "#                                                      0,\n",
    "#                                                      1,\n",
    "#                                                      1,\n",
    "#                                                      1]]\n",
    "\n",
    "Y = np.array([[0,0,0,1,1,1]]).T\n",
    "# print Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Random seeding \n",
    "\n",
    "- Seeding is a numpy method so that there is a uniform random distribution of numbers everytime the network is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Defining a weight matrix for this Neural Network :\n",
    "\n",
    "- Because there are only 2 layers in the neural network, the input layer & the output layer, only one weight matrix is necessary to connect the two layers.\n",
    "\n",
    "- The dimension of the weight matrix is 3 X 1 because the input layer(as discussed in step 3) is of size 3 and the output layer(as discussed in step 4) is a size of 1.\n",
    "\n",
    "- The initialization of the weight is very theory intensive, but as best practice for this simple example, we initialize the weight randomly with a zero mean.\n",
    "    ##### \"zero mean\" means that the sum of the entries vector divided by the dimension of the matrix equates to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing the weight matrix randomly.\n",
    "weightMatrix = 2*np.random.random((3,1)) - 1\n",
    "# print weightMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Training the Neural Network :\n",
    "\n",
    "- We create a \"for\" loop that iterates many times over our training data defined in step 3 & 4 to optimize the network to the specified dataset.\n",
    "\n",
    "### Forward Propagation :\n",
    "\n",
    "- We are going to utilize a technique known as full-batch training which will process all the input data(defined as X in step 3) at the same time.\n",
    "\n",
    "- We then proceed towards letting the netword predict (I use the term predict loosely because each prediction is a constant iteration towards an even better prediction) the output based on the input.\n",
    "\n",
    "- The next bit of code below can be interpreted in 2 steps. Firstly, our inputLayer matrix, which is X(defined in step 3) of size 6 X 3 undergoes dot product (matrix multiplication) with our initialized weightMatrix(from step 6) of size 3 X 1. As a rule, the result of the dot product is a matrix of size 6 X 1.\n",
    "```python \n",
    "outputLayer = sigmoidFn(np.dot(inputLayer,weightMatrix))\n",
    "```\n",
    "\n",
    "- Another important thing occurance to note in the above line of code is that the output of the dot product passes through the sigmoidFn declared way up in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT\n",
      "[[  1.28236047e-06]\n",
      " [  1.14091723e-02]\n",
      " [  1.14091723e-02]\n",
      " [  9.90463826e-01]\n",
      " [  9.90463826e-01]\n",
      " [  9.99999988e-01]]\n"
     ]
    }
   ],
   "source": [
    "for iter in xrange(10):\n",
    "\n",
    "    # forward propagation\n",
    "    inputLayer = X\n",
    "    outputLayer = sigmoidFn(np.dot(inputLayer,weightMatrix))\n",
    "#         print \"W-Mtrix:\",weightMatrix\n",
    "#         print \"iputLayer\",inputLayer\n",
    "#         print \"DotProd\",np.dot(inputLayer, weightMatrix)\n",
    "    \n",
    "                #################\n",
    "                #               #\n",
    "                # Read STEP : 8 #\n",
    "                #               #\n",
    "                #################\n",
    "    outputLayerError = Y - outputLayer\n",
    "#         print \"OutputLayerError: \",outputLayerError\n",
    "\n",
    "                #################\n",
    "                #               #\n",
    "                # Read STEP : 9 #\n",
    "                #               #\n",
    "                #################\n",
    "    errorBasedChange = outputLayerError * sigmoidFn(outputLayer, True) \n",
    "#         print \"Derivative of the outputLayer:\", sigmoidFn(outputLayer, True)\n",
    "#         print \"Change based on outputLayerError:\",errorBasedChange\n",
    "        \n",
    "        \n",
    "                #################\n",
    "                #               #\n",
    "                # Read STEP : 10 #\n",
    "                #               #\n",
    "                #################\n",
    "    weightMatrix += np.dot(inputLayer.T, errorBasedChange)\n",
    "#     print \"T\",inputLayer.T\n",
    "#     print \"E\",errorBasedChange\n",
    "#     print \"W\",weightMatrix\n",
    "#         print \"Dot Product:\", np.dot(inputLayer.T, errorBasedChange)\n",
    "#         print \"Updated Weight Matrix\", weightMatrix\n",
    "#         print \"Outputs after each iterations:\", outputLayer\n",
    "\n",
    "print \"OUTPUT\"\n",
    "print outputLayer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 8 - Evaluating calculation error :\n",
    "\n",
    "\n",
    "- The line of code below is simply a means to calculate the accuracy of our predicted value(outputLayer) compared to the actual, predefined output value (Y).\n",
    "- Remember, that Y is a 6 X 1 matrix(As defined in part 4). The outputLayer is also a 6 X 1 matrix, therefore it is fairly intuitive that in order to calculate the error, we would have to subtract the two matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Calculating the \"Error Based Change\" :\n",
    "\n",
    "- To understand this step fully, we can take it step-by-step.\n",
    "    - Firstly, after calculating the derivative sigmoidFn(outputLayer, True), each calculated value is always between 0 & 1. That is a property of the sigmoid function.\n",
    "    - Secondly, we multiply the returned 6 X 1 derivative consisting matrix above to the outputLayerError, which is also a 6 X 1 matrix.\n",
    "###### What the second step does is that it multiplies the derivatves to the error \"elementwise\" and that helps to reduce the errors of predictions that carry a high confidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Updating the weight matrix based on previously achieved results :\n",
    "\n",
    "- This update step is highly dependent on how much of a difference there is between the actual output(Y) and the estimated output(outputLayer).\n",
    "- If one thinks about it intuitively, it makes sense that if a weight is already accurately predicting the correct output value, it does not need to be tampered with much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a 3 Layer Neural Network using the methodology defined in steps 1 through 10.\n",
    "\n",
    "#### Uncomment the print statements at the end of the code to see step-by-step what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoidFn(x, deriv=False):\n",
    "    if deriv == True:\n",
    "        return x * (1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "# 4 X 3 Matrix\n",
    "X = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])\n",
    "\n",
    "# 4 X 1 Matrix\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# To make sure all our randomly generated values are the same in all runs\n",
    "np.random.seed(1)\n",
    "\n",
    "# 3 X 4 Matrix\n",
    "weightMatrix1 = 2 * np.random.random((3,4)) - 1\n",
    "# 4 X 1 Matrix\n",
    "weightMatrix2 = 2 * np.random.random((4,1)) - 1\n",
    "\n",
    "for j in xrange(60000):\n",
    "    inputLayer = X\n",
    "#     We estimate the hiddenLayer\n",
    "    hiddenLayer = sigmoidFn(np.dot(inputLayer, weightMatrix1))\n",
    "#     We estimate the outputLayer\n",
    "    outputLayer = sigmoidFn(np.dot(hiddenLayer, weightMatrix2))\n",
    "    \n",
    "#     How much is our estimation off by for the outputLayer\n",
    "    outputLayerError = y - outputLayer\n",
    "    \n",
    "#     In what direction is the target value outputLayer? How accurate was our estimation\n",
    "    outputLayerErrorBasedChange = outputLayerError * sigmoidFn(outputLayer, True)\n",
    "    \n",
    "#     How much did each hiddenLayerValue Contribute to the outputLayerError?\n",
    "# Back Propagation occurs here\n",
    "    hiddenLayerError = outputLayerErrorBasedChange.dot(weightMatrix2.T)\n",
    "    \n",
    "#     In what direction is target value hiddenLayer and how accurate was the estimation\n",
    "    hiddenLayerErrorBasedChange = hiddenLayerError * sigmoidFn(hiddenLayer, True)\n",
    "    \n",
    "    weightMatrix2 += hiddenLayer.T.dot(outputLayerErrorBasedChange)\n",
    "    weightMatrix1 += inputLayer.T.dot(hiddenLayerErrorBasedChange)\n",
    "    \n",
    "# When Running This Kernel, please allow a couple of seconds for the iteration to complete\n",
    "\n",
    "# print \"InputLayer\"\n",
    "# print inputLayer\n",
    "# print \"-------------\"\n",
    "# print \"weightMatrix1\"\n",
    "# print weightMatrix1\n",
    "# print \"-------------\"\n",
    "# print \"dot Product of InputLayer & weightMatrix1\"\n",
    "# print np.dot(inputLayer, weightMatrix1)\n",
    "# print \"-------------\"\n",
    "# print \"HiddenLayer\"\n",
    "# print hiddenLayer\n",
    "# print \"-------------\"\n",
    "# print \"weightMatrix2\"\n",
    "# print weightMatrix2\n",
    "# print \"-------------\"\n",
    "# print \"dot Product of HiddenLayer & WeightMatrix2\"\n",
    "# print np.dot(hiddenLayer, weightMatrix2)\n",
    "# print \"-------------\"\n",
    "# print \"OutputLayer:\"\n",
    "# print outputLayer\n",
    "# print \"OutputLayerError:\"\n",
    "# print outputLayerError\n",
    "# print \"-------------\"\n",
    "# print \"OutputLayerErrorBasedChange:\"\n",
    "# print outputLayerErrorBasedChange\n",
    "# print \"-------------\"\n",
    "# print \"HiddenLayerError:\"\n",
    "# print hiddenLayerError\n",
    "# print \"-------------\"\n",
    "# print \"HiddenLayerErrorBasedChange:\"\n",
    "# print hiddenLayerErrorBasedChange\n",
    "# print \"-------------------------------\"\n",
    "# print \"MEAN OUTPUT ERROR\"\n",
    "# print str(np.mean(np.abs(outputLayerError)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
